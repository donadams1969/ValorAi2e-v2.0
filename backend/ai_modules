{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNMiqSpZWkLqG0W3oa3RIOe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/donadams1969/ValorAi2e-v2.0/blob/main/backend/ai_modules\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "taGl7W1msiCe"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import json\n",
        "import openai\n",
        "import functools\n",
        "import hvac\n",
        "import redis\n",
        "import structlog\n",
        "from prometheus_client import Counter, Histogram, start_http_server\n",
        "from pybreaker import CircuitBreaker, CircuitRedisStorage\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential\n",
        "from scrubadubdub import Scrub\n",
        "\n",
        "# ================== Configuration ==================\n",
        "class APIConfig:\n",
        "    CIRCUIT_FAIL_MAX = 5\n",
        "    CIRCUIT_RESET_TIMEOUT = 60\n",
        "    PROMETHEUS_PORT = 9090\n",
        "    AUDIT_LOG_FILE = \"/var/log/ai_service/audit.log\"\n",
        "    VAULT_ADDR = os.getenv(\"VAULT_ADDR\", \"http://localhost:8200\")\n",
        "    VAULT_TOKEN = os.getenv(\"VAULT_TOKEN\", None)\n",
        "\n",
        "# ================== Monitoring Setup ==================\n",
        "API_CALLS = Counter('ai_service_api_calls', 'Total API calls', ['operation', 'model'])\n",
        "API_ERRORS = Counter('ai_service_api_errors', 'API errors', ['error_type'])\n",
        "API_LATENCY = Histogram('ai_service_api_latency', 'API latency distribution', ['operation'])\n",
        "\n",
        "# ================== Circuit Breaker Setup ==================\n",
        "redis_client = redis.StrictRedis()\n",
        "storage = CircuitRedisStorage(redis_client)\n",
        "ai_circuit = CircuitBreaker(\n",
        "    fail_max=APIConfig.CIRCUIT_FAIL_MAX,\n",
        "    reset_timeout=APIConfig.CIRCUIT_RESET_TIMEOUT,\n",
        "    state_storage=storage\n",
        ")\n",
        "\n",
        "# ================== Logging Setup ==================\n",
        "scrubber = Scrub()\n",
        "structlog.configure(\n",
        "    processors=[\n",
        "        structlog.processors.add_log_level,\n",
        "        structlog.processors.TimeStamper(fmt=\"iso\"),\n",
        "        structlog.processors.JSONRenderer(),\n",
        "        scrubber.scrub\n",
        "    ],\n",
        "    context_class=dict,\n",
        "    logger_factory=structlog.PrintLoggerFactory()\n",
        ")\n",
        "logger = structlog.get_logger()\n",
        "audit_logger = structlog.PrintLoggerFactory()(file=open(APIConfig.AUDIT_LOG_FILE, \"a\"))\n",
        "\n",
        "# ================== Vault Secret Retrieval ==================\n",
        "def get_vault_secret(path: str) -> str:\n",
        "    client = hvac.Client(url=APIConfig.VAULT_ADDR, token=APIConfig.VAULT_TOKEN)\n",
        "    secret = client.secrets.kv.v2.read_secret_version(path=path)\n",
        "    return secret['data']['data']['value']\n",
        "\n",
        "# ================== Core Class ==================\n",
        "class OpenAIManager:\n",
        "    def __init__(self,\n",
        "                 api_key: Optional[str] = None,\n",
        "                 default_model: str = \"gpt-4-turbo\",\n",
        "                 max_retries: int = 3,\n",
        "                 initial_delay: float = 1.0):\n",
        "        # Load API key securely\n",
        "        self._api_key = api_key or get_vault_secret(\"openai/api-key\")\n",
        "        openai.api_key = self._api_key\n",
        "\n",
        "        self.default_model = default_model\n",
        "        self.max_retries = max_retries\n",
        "        self.initial_delay = initial_delay\n",
        "\n",
        "        # Start Prometheus metrics server\n",
        "        start_http_server(APIConfig.PROMETHEUS_PORT)\n",
        "\n",
        "        # Cache for prompt responses\n",
        "        self._cached_generate_text = functools.lru_cache(maxsize=128)(self._generate_text_uncached)\n",
        "\n",
        "    @ai_circuit\n",
        "    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))\n",
        "    def get_chat_completion(self,\n",
        "                            messages: list,\n",
        "                            model: Optional[str] = None,\n",
        "                            temperature: float = 0.7,\n",
        "                            max_tokens: Optional[int] = None,\n",
        "                            stream: bool = False) -> Optional[Union[str, any]]:\n",
        "        \"\"\"\n",
        "        Send chat messages to OpenAI API with retries and metrics.\n",
        "        \"\"\"\n",
        "        start_time = time.time()\n",
        "        current_model = model or self.default_model\n",
        "        operation = f\"chat_completion_{current_model}\"\n",
        "\n",
        "        try:\n",
        "            with API_LATENCY.labels(operation=operation).time():\n",
        "                response = openai.ChatCompletion.create(\n",
        "                    model=current_model,\n",
        "                    messages=messages,\n",
        "                    temperature=temperature,\n",
        "                    max_tokens=max_tokens,\n",
        "                    stream=stream\n",
        "                )\n",
        "                API_CALLS.labels(operation=operation, model=current_model).inc()\n",
        "\n",
        "                if stream:\n",
        "                    return response\n",
        "                elif response.choices and response.choices[0].message:\n",
        "                    return response.choices[0].message.content.strip()\n",
        "                else:\n",
        "                    logger.warning(\"Empty API response\", model=current_model)\n",
        "                    return None\n",
        "        except Exception as e:\n",
        "            API_ERRORS.labels(error_type=type(e).__name__).inc()\n",
        "            self._handle_api_error(e)\n",
        "            raise\n",
        "        finally:\n",
        "            duration = time.time() - start_time\n",
        "            logger.info(\"API request completed\",\n",
        "                        model=current_model,\n",
        "                        duration=duration,\n",
        "                        operation=operation)\n",
        "\n",
        "    def _handle_api_error(self, e: Exception):\n",
        "        error_type = type(e).__name__\n",
        "        audit_logger.error(\"API security event\",\n",
        "                           event_type=\"api_error\",\n",
        "                           error=error_type,\n",
        "                           timestamp=time.time(),\n",
        "                           user_id=os.getenv(\"USER_ID\", \"system\"))\n",
        "        if isinstance(e, openai.error.AuthenticationError):\n",
        "            logger.critical(\"Authentication failure\", error=str(e))\n",
        "        elif isinstance(e, openai.error.RateLimitError):\n",
        "            logger.warning(\"Rate limit exceeded\", error=str(e))\n",
        "        else:\n",
        "            logger.error(\"API operation failed\", error=str(e))\n",
        "\n",
        "    def _generate_text_uncached(self,\n",
        "                                prompt: str,\n",
        "                                model: Optional[str] = None,\n",
        "                                temperature: float = 0.7,\n",
        "                                max_tokens: Optional[int] = None) -> Optional[str]:\n",
        "        \"\"\"\n",
        "        Internal method to generate text, used by cache.\n",
        "        \"\"\"\n",
        "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "        return self.get_chat_completion(messages, model, temperature, max_tokens)\n",
        "\n",
        "    def generate_text(self,\n",
        "                      prompt: str,\n",
        "                      model: Optional[str] = None,\n",
        "                      temperature: float = 0.7,\n",
        "                      max_tokens: Optional[int] = None) -> Optional[str]:\n",
        "        \"\"\"\n",
        "        Generate response with caching for repeated prompts.\n",
        "        \"\"\"\n",
        "        return self._cached_generate_text(prompt, model, temperature, max_tokens)\n",
        "\n",
        "    def clear_cache(self):\n",
        "        \"\"\"Clear cached responses.\"\"\"\n",
        "        self._cached_generate_text.cache_clear()\n",
        "        logger.info(\"Response cache cleared.\")\n",
        "\n",
        "# ================== Usage Example ==================\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        manager = OpenAIManager()\n",
        "        # Example: Basic prompt\n",
        "        response = manager.get_chat_completion(\n",
        "            [{\"role\": \"user\", \"content\": \"Explain quantum computing\"}]\n",
        "        )\n",
        "        print(\"Response:\", response)\n",
        "\n",
        "        # Example: Using generate_text with caching\n",
        "        prompt = \"What is the capital of Canada?\"\n",
        "        print(\"First call (API):\")\n",
        "        print(manager.generate_text(prompt))\n",
        "        print(\"Second call (cached):\")\n",
        "        print(manager.generate_text(prompt))\n",
        "        print(\"Clearing cache and re-asking:\")\n",
        "        manager.clear_cache()\n",
        "        print(manager.generate_text(prompt))\n",
        "    except Exception as e:\n",
        "        logger.exception(\"An error occurred during execution.\")\n"
      ]
    }
  ]
}